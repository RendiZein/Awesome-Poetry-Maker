{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RendiZein/Awesome-Poetry-Maker/blob/main/Awesome_Poetry_Maker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFWbEb6uGbN-"
      },
      "source": [
        "# Awesome Poetry\n",
        "\n",
        "Create a model that will predict the next word in a text sequence, i implement such model and train it using a corpus of Shakespeare's sonnets, while also creating some helper functions to pre-process the data and achive 80% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BOwsuGQQY9OL"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.utils as ku "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTxqlHqKHzhr"
      },
      "source": [
        "I will be using the [Shakespeare Sonnets Dataset](https://www.opensourceshakespeare.org/views/sonnets/sonnet_view.php?range=viewrange&sonnetrange1=1&sonnetrange2=154), which contains more than 2000 lines of text extracted from Shakespeare's sonnets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ4qOUzujMP6",
        "outputId": "291f6cea-c220-49b5-eb2f-796c67addbe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=108jAePKK4R3BVYBbYJZ32JWUwxeMg20K\n",
            "To: /content/sonnets.txt\n",
            "100% 93.6k/93.6k [00:00<00:00, 97.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# sonnets.txt\n",
        "!gdown --id 108jAePKK4R3BVYBbYJZ32JWUwxeMg20K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pfd-nYKij5yY",
        "outputId": "15e450c9-2c30-45c5-8d6d-82b6f97469fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2159 lines of sonnets\n",
            "\n",
            "The first 5 lines look like this:\n",
            "\n",
            "from fairest creatures we desire increase,\n",
            "that thereby beauty's rose might never die,\n",
            "but as the riper should by time decease,\n",
            "his tender heir might bear his memory:\n",
            "but thou, contracted to thine own bright eyes,\n"
          ]
        }
      ],
      "source": [
        "# Define path for file with sonnets\n",
        "SONNETS_FILE = './sonnets.txt'\n",
        "\n",
        "# Read the data\n",
        "with open('./sonnets.txt') as f:\n",
        "    data = f.read()\n",
        "\n",
        "# Convert to lower case and save as a list\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "print(f\"There are {len(corpus)} lines of sonnets\\n\")\n",
        "print(f\"The first 5 lines look like this:\\n\")\n",
        "for i in range(5):\n",
        "  print(corpus[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imB15zrSNhA1"
      },
      "source": [
        "## Tokenizing the text\n",
        "\n",
        "Fit the Tokenizer to the corpus and save the total number of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AAhM_qAZk0o5"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77-0sA46OETa"
      },
      "source": [
        "converting the text into sequences with `texts_to_sequences`.\n",
        "\n",
        "Process this corpus one line at a time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPZmZtpEPEeI"
      },
      "source": [
        "To avoid unexpected result because `texts_to_sequences` expects a list and i am providing a string. A string is still and `iterable` in Python so i will get the word index of every character in the string."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oqy9KjXRJ9A"
      },
      "source": [
        "## Generating n_grams\n",
        "\n",
        "Now complete the `n_gram_seqs` function below. This function receives the fitted tokenizer and the corpus (which is a list of strings) and should return a list containing the `n_gram` sequences for each line in the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iy4baJMDl6kj"
      },
      "outputs": [],
      "source": [
        "def n_gram_seqs(corpus, tokenizer):\n",
        "\tinput_sequences = []\n",
        "\n",
        "\tfor line in corpus:\n",
        "\n",
        "\t\t# Tokenize the current line\n",
        "\t\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\n",
        "\t\t# Loop over the line several times to generate the subphrases\n",
        "\t\tfor i in range(1, len(token_list)):\n",
        "\t\t\t\n",
        "\t\t\t# Generate the subphrase\n",
        "\t\t\tn_gram_sequence = token_list[:i+1]\n",
        "\n",
        "\t\t\t# Append the subphrase to the sequences list\n",
        "\t\t\t# Pad all sequences\n",
        "\t\t\tinput_sequences.append(n_gram_sequence)\n",
        "\t \n",
        "\treturn input_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlKqW2pfM7G3",
        "outputId": "c6a16a93-805b-4938-aa78-b24b88893aed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_gram sequences for first example look like this:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[34, 417],\n",
              " [34, 417, 877],\n",
              " [34, 417, 877, 166],\n",
              " [34, 417, 877, 166, 213],\n",
              " [34, 417, 877, 166, 213, 517]]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Test your function with one example\n",
        "first_example_sequence = n_gram_seqs([corpus[0]], tokenizer)\n",
        "\n",
        "print(\"n_gram sequences for first example look like this:\\n\")\n",
        "first_example_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtPpCcBjNc4c",
        "outputId": "88c04fed-e35f-47a8-f9d8-c6f62ce569c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_gram sequences for next 3 examples look like this:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[8, 878],\n",
              " [8, 878, 134],\n",
              " [8, 878, 134, 351],\n",
              " [8, 878, 134, 351, 102],\n",
              " [8, 878, 134, 351, 102, 156],\n",
              " [8, 878, 134, 351, 102, 156, 199],\n",
              " [16, 22],\n",
              " [16, 22, 2],\n",
              " [16, 22, 2, 879],\n",
              " [16, 22, 2, 879, 61],\n",
              " [16, 22, 2, 879, 61, 30],\n",
              " [16, 22, 2, 879, 61, 30, 48],\n",
              " [16, 22, 2, 879, 61, 30, 48, 634],\n",
              " [25, 311],\n",
              " [25, 311, 635],\n",
              " [25, 311, 635, 102],\n",
              " [25, 311, 635, 102, 200],\n",
              " [25, 311, 635, 102, 200, 25],\n",
              " [25, 311, 635, 102, 200, 25, 278]]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Test the function with a bigger corpus\n",
        "next_3_examples_sequence = n_gram_seqs(corpus[1:4], tokenizer)\n",
        "\n",
        "print(\"n_gram sequences for next 3 examples look like this:\\n\")\n",
        "next_3_examples_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx3V_RjFWQSu"
      },
      "source": [
        "Apply the `n_gram_seqs` transformation to the whole corpus and save the maximum sequence length to use it later:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laMwiRUpmuSd",
        "outputId": "8fb9103b-0e83-449f-9126-0300866323c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_grams of input_sequences have length: 15462\n",
            "maximum length of sequences is: 11\n"
          ]
        }
      ],
      "source": [
        "# Apply the n_gram_seqs transformation to the whole corpus\n",
        "input_sequences = n_gram_seqs(corpus, tokenizer)\n",
        "\n",
        "# Save max length \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "\n",
        "print(f\"n_grams of input_sequences have length: {len(input_sequences)}\")\n",
        "print(f\"maximum length of sequences is: {max_sequence_len}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHY7HroqWq12"
      },
      "source": [
        "## Add padding to the sequences\n",
        "\n",
        "Now code the `pad_seqs` function which will pad any given sequences to the desired maximum length. Notice that this function receives a list of sequences and should return a numpy array with the padded sequences: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "code",
        "id": "WW1-qAZaWOhC"
      },
      "outputs": [],
      "source": [
        "def pad_seqs(input_sequences, maxlen):\n",
        "    padded_sequences = np.array(pad_sequences(input_sequences, maxlen=maxlen, padding='pre'))\n",
        "    \n",
        "    return padded_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqVQ0pb3YHLr",
        "outputId": "c7ab783c-08ae-46b7-90cc-f7e8e3ea80e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,  34, 417],\n",
              "       [  0,   0,  34, 417, 877],\n",
              "       [  0,  34, 417, 877, 166],\n",
              "       [ 34, 417, 877, 166, 213],\n",
              "       [417, 877, 166, 213, 517]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Test the function with the n_grams_seq of the first example\n",
        "first_padded_seq = pad_seqs(first_example_sequence, len(first_example_sequence))\n",
        "first_padded_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j56_UCOBYzZt",
        "outputId": "f014a632-0b7e-41d8-b5be-8341ed44ec49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   8, 878],\n",
              "       [  0,   0,   0,   0,   0,   8, 878, 134],\n",
              "       [  0,   0,   0,   0,   8, 878, 134, 351],\n",
              "       [  0,   0,   0,   8, 878, 134, 351, 102],\n",
              "       [  0,   0,   8, 878, 134, 351, 102, 156],\n",
              "       [  0,   8, 878, 134, 351, 102, 156, 199],\n",
              "       [  0,   0,   0,   0,   0,   0,  16,  22],\n",
              "       [  0,   0,   0,   0,   0,  16,  22,   2],\n",
              "       [  0,   0,   0,   0,  16,  22,   2, 879],\n",
              "       [  0,   0,   0,  16,  22,   2, 879,  61],\n",
              "       [  0,   0,  16,  22,   2, 879,  61,  30],\n",
              "       [  0,  16,  22,   2, 879,  61,  30,  48],\n",
              "       [ 16,  22,   2, 879,  61,  30,  48, 634],\n",
              "       [  0,   0,   0,   0,   0,   0,  25, 311],\n",
              "       [  0,   0,   0,   0,   0,  25, 311, 635],\n",
              "       [  0,   0,   0,   0,  25, 311, 635, 102],\n",
              "       [  0,   0,   0,  25, 311, 635, 102, 200],\n",
              "       [  0,   0,  25, 311, 635, 102, 200,  25],\n",
              "       [  0,  25, 311, 635, 102, 200,  25, 278]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Test the function with the n_grams_seq of the next 3 examples\n",
        "next_3_padded_seq = pad_seqs(next_3_examples_sequence, max([len(s) for s in next_3_examples_sequence]))\n",
        "next_3_padded_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgK-Q_micEYA",
        "outputId": "5cd8416d-a054-4d93-f8b3-9763a53a6e76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "padded corpus has shape: (15462, 11)\n"
          ]
        }
      ],
      "source": [
        "# Pad the whole corpus\n",
        "input_sequences = pad_seqs(input_sequences, max_sequence_len)\n",
        "\n",
        "print(f\"padded corpus has shape: {input_sequences.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbOidyPrXxf7"
      },
      "source": [
        "## Split the data into features and labels\n",
        "\n",
        "Before feeding the data into the neural network it should split it into features and labels. In this case the features will be the padded n_gram sequences with the last word removed from them and the labels will be the removed word.\n",
        "\n",
        "`features_and_labels` function expects the padded n_gram sequences as input and return a tuple containing the features and the one hot encoded labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cellView": "code",
        "id": "9WGGbYdnZdmJ"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: features_and_labels\n",
        "def features_and_labels(input_sequences, total_words):\n",
        "    features = input_sequences[:,:-1]\n",
        "    labels = input_sequences[:,-1]\n",
        "    one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
        "\n",
        "    return features, one_hot_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23DolaBRaIAZ",
        "outputId": "4875c124-5d75-43fb-ce03-52192d3fc011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels have shape: (5, 3211)\n",
            "\n",
            "features look like this:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,  34],\n",
              "       [  0,   0,  34, 417],\n",
              "       [  0,  34, 417, 877],\n",
              "       [ 34, 417, 877, 166],\n",
              "       [417, 877, 166, 213]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Test the function with the padded n_grams_seq of the first example\n",
        "first_features, first_labels = features_and_labels(first_padded_seq, total_words)\n",
        "\n",
        "print(f\"labels have shape: {first_labels.shape}\")\n",
        "print(\"\\nfeatures look like this:\\n\")\n",
        "first_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRTuLEt3bRKa",
        "outputId": "ceabe5ae-fbc3-412c-f753-fde2ba5b29de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features have shape: (15462, 10)\n",
            "labels have shape: (15462, 3211)\n"
          ]
        }
      ],
      "source": [
        "# Split the whole corpus\n",
        "features, labels = features_and_labels(input_sequences, total_words)\n",
        "\n",
        "print(f\"features have shape: {features.shape}\")\n",
        "print(f\"labels have shape: {labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltxaOCE_aU6J"
      },
      "source": [
        "## Create the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "cellView": "code",
        "id": "XrE6kpJFfvRY"
      },
      "outputs": [],
      "source": [
        "from types import DynamicClassAttribute\n",
        "def create_model(total_words, max_sequence_len):\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Embedding(total_words, 100, input_length = max_sequence_len - 1)) # Your Embedding Layer\n",
        "    model.add(Bidirectional(LSTM(150, return_sequences = True))) # An LSTM Layer\n",
        "    model.add(Dropout(0.2)) # A dropout layer\n",
        "    model.add(LSTM(100)) # Another LSTM Layer\n",
        "    model.add(Dense(total_words/2, activation = 'relu', kernel_regularizer = regularizers.l2(0.01))) # A Dense Layer including regularizers\n",
        "    model.add(Dense(total_words, activation = 'softmax')) # A Dense Layer\n",
        "    # Pick an optimizer \n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy']) # Pick a loss function and an optimizer\n",
        "    print(model.summary())\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_model1(total_words, max_sequence_len):\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Embedding(total_words, 100, input_length = max_sequence_len - 1)) # Your Embedding Layer\n",
        "    model.add(Bidirectional(LSTM(175, return_sequences = True))) # An LSTM Layer\n",
        "    model.add(Dropout(0.15)) # A dropout layer\n",
        "    model.add(LSTM(125)) # Another LSTM Layer\n",
        "    model.add(Dense(total_words/2, activation = 'relu', kernel_regularizer = regularizers.l2(0.01))) # A Dense Layer including regularizers\n",
        "    model.add(Dense(total_words, activation = 'softmax')) # A Dense Layer\n",
        "    # Pick an optimizer \n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy']) # Pick a loss function and an optimizer\n",
        "    print(model.summary())\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_model2(total_words, max_sequence_len):\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Embedding(total_words, 100, input_length = max_sequence_len - 1)) # Your Embedding Layer\n",
        "    model.add(Bidirectional(LSTM(150, return_sequences = True))) # An LSTM Layer\n",
        "    model.add(Dropout(0.3)) # A dropout layer\n",
        "    model.add(LSTM(100)) # Another LSTM Layer\n",
        "    model.add(Dense(total_words/2, activation = 'relu', kernel_regularizer = regularizers.l2(0.01))) # A Dense Layer including regularizers\n",
        "    model.add(Dense(total_words, activation = 'softmax')) # A Dense Layer\n",
        "    # Pick an optimizer \n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy']) # Pick a loss function and an optimizer\n",
        "    print(model.summary())\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_model3(total_words, max_sequence_len):\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Embedding(total_words, 100, input_length = max_sequence_len - 1)) # Your Embedding Layer\n",
        "    model.add(Bidirectional(LSTM(150, return_sequences = True))) # An LSTM Layer\n",
        "    model.add(Dropout(0.25)) # A dropout layer\n",
        "    model.add(LSTM(150)) # Another LSTM Layer\n",
        "    model.add(Dense(total_words/2, activation = 'relu', kernel_regularizer = regularizers.l2(0.01))) # A Dense Layer including regularizers\n",
        "    model.add(Dense(total_words, activation = 'softmax')) # A Dense Layer\n",
        "    # Pick an optimizer \n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy']) # Pick a loss function and an optimizer\n",
        "    print(model.summary())\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "JiMEy-OTSg2v"
      },
      "outputs": [],
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    '''\n",
        "    Halts the training after reaching 60 percent accuracy\n",
        "\n",
        "    Args:\n",
        "      epoch (integer) - index of epoch (required but unused in the function definition below)\n",
        "      logs (dict) - metric results from the training epoch\n",
        "    '''\n",
        "\n",
        "    # Check accuracy\n",
        "    if(logs.get('accuracy') > 0.99):\n",
        "\n",
        "      # Stop if threshold is met\n",
        "      print(\"\\nAccuracy is higher than 0.8 so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "# Instantiate class\n",
        "callbacks = myCallback()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IpX_Gu_gISk",
        "outputId": "409b642a-c15b-4b28-d23c-4ed5273a1b59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 10, 100)           321100    \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 10, 300)          301200    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 10, 300)           0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1605)              162105    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3211)              5156866   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,101,671\n",
            "Trainable params: 6,101,671\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 10, 100)           321100    \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 10, 300)          301200    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 10, 300)           0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1605)              162105    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3211)              5156866   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,101,671\n",
            "Trainable params: 6,101,671\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/250\n",
            "484/484 [==============================] - 13s 10ms/step - loss: 6.8985 - accuracy: 0.0204\n",
            "Epoch 2/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 6.4993 - accuracy: 0.0218\n",
            "Epoch 3/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 6.3898 - accuracy: 0.0260\n",
            "Epoch 4/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 6.2731 - accuracy: 0.0298\n",
            "Epoch 5/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 6.1864 - accuracy: 0.0355\n",
            "Epoch 6/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 6.1073 - accuracy: 0.0367\n",
            "Epoch 7/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 6.0295 - accuracy: 0.0409\n",
            "Epoch 8/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 5.9494 - accuracy: 0.0420\n",
            "Epoch 9/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 5.8576 - accuracy: 0.0510\n",
            "Epoch 10/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 5.7523 - accuracy: 0.0544\n",
            "Epoch 11/250\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 5.6390 - accuracy: 0.0622\n",
            "Epoch 12/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 5.5260 - accuracy: 0.0697\n",
            "Epoch 13/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 5.4139 - accuracy: 0.0740\n",
            "Epoch 14/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 5.3007 - accuracy: 0.0805\n",
            "Epoch 15/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 5.1960 - accuracy: 0.0854\n",
            "Epoch 16/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 5.0859 - accuracy: 0.0968\n",
            "Epoch 17/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 4.9852 - accuracy: 0.1008\n",
            "Epoch 18/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 4.8781 - accuracy: 0.1079\n",
            "Epoch 19/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 4.7821 - accuracy: 0.1168\n",
            "Epoch 20/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 4.6748 - accuracy: 0.1222\n",
            "Epoch 21/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 4.5742 - accuracy: 0.1338\n",
            "Epoch 22/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 4.4682 - accuracy: 0.1425\n",
            "Epoch 23/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 4.3699 - accuracy: 0.1546\n",
            "Epoch 24/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 4.2624 - accuracy: 0.1696\n",
            "Epoch 25/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 4.1622 - accuracy: 0.1780\n",
            "Epoch 26/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 4.0642 - accuracy: 0.1943\n",
            "Epoch 27/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.9630 - accuracy: 0.2072\n",
            "Epoch 28/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.8659 - accuracy: 0.2299\n",
            "Epoch 29/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.7682 - accuracy: 0.2432\n",
            "Epoch 30/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.6760 - accuracy: 0.2595\n",
            "Epoch 31/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.5747 - accuracy: 0.2813\n",
            "Epoch 32/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.5048 - accuracy: 0.2963\n",
            "Epoch 33/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 3.4122 - accuracy: 0.3130\n",
            "Epoch 34/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.3224 - accuracy: 0.3375\n",
            "Epoch 35/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.2523 - accuracy: 0.3531\n",
            "Epoch 36/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.1608 - accuracy: 0.3724\n",
            "Epoch 37/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 3.0871 - accuracy: 0.3884\n",
            "Epoch 38/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 3.0227 - accuracy: 0.4031\n",
            "Epoch 39/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.9544 - accuracy: 0.4181\n",
            "Epoch 40/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.8782 - accuracy: 0.4320\n",
            "Epoch 41/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.8124 - accuracy: 0.4547\n",
            "Epoch 42/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.7464 - accuracy: 0.4637\n",
            "Epoch 43/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.6797 - accuracy: 0.4831\n",
            "Epoch 44/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.6120 - accuracy: 0.4961\n",
            "Epoch 45/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.5780 - accuracy: 0.5040\n",
            "Epoch 46/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.5163 - accuracy: 0.5185\n",
            "Epoch 47/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.4501 - accuracy: 0.5354\n",
            "Epoch 48/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.3866 - accuracy: 0.5493\n",
            "Epoch 49/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.3493 - accuracy: 0.5565\n",
            "Epoch 50/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.3097 - accuracy: 0.5645\n",
            "Epoch 51/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.2557 - accuracy: 0.5771\n",
            "Epoch 52/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.1969 - accuracy: 0.5962\n",
            "Epoch 53/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.1682 - accuracy: 0.5980\n",
            "Epoch 54/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.1262 - accuracy: 0.6057\n",
            "Epoch 55/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.0764 - accuracy: 0.6142\n",
            "Epoch 56/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.0331 - accuracy: 0.6306\n",
            "Epoch 57/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.0012 - accuracy: 0.6356\n",
            "Epoch 58/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.9646 - accuracy: 0.6434\n",
            "Epoch 59/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.9314 - accuracy: 0.6513\n",
            "Epoch 60/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.8954 - accuracy: 0.6596\n",
            "Epoch 61/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.8573 - accuracy: 0.6652\n",
            "Epoch 62/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.8268 - accuracy: 0.6744\n",
            "Epoch 63/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.7808 - accuracy: 0.6823\n",
            "Epoch 64/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.7657 - accuracy: 0.6888\n",
            "Epoch 65/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.7353 - accuracy: 0.6911\n",
            "Epoch 66/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.7056 - accuracy: 0.7000\n",
            "Epoch 67/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.6811 - accuracy: 0.7022\n",
            "Epoch 68/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.6542 - accuracy: 0.7088\n",
            "Epoch 69/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.6285 - accuracy: 0.7156\n",
            "Epoch 70/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.5902 - accuracy: 0.7267\n",
            "Epoch 71/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.5740 - accuracy: 0.7250\n",
            "Epoch 72/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.5576 - accuracy: 0.7308\n",
            "Epoch 73/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.5407 - accuracy: 0.7310\n",
            "Epoch 74/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.5387 - accuracy: 0.7305\n",
            "Epoch 75/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.5009 - accuracy: 0.7409\n",
            "Epoch 76/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4736 - accuracy: 0.7498\n",
            "Epoch 77/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.4500 - accuracy: 0.7528\n",
            "Epoch 78/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.4332 - accuracy: 0.7535\n",
            "Epoch 79/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.4239 - accuracy: 0.7530\n",
            "Epoch 80/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3937 - accuracy: 0.7606\n",
            "Epoch 81/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.3802 - accuracy: 0.7640\n",
            "Epoch 82/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.3804 - accuracy: 0.7603\n",
            "Epoch 83/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.3472 - accuracy: 0.7704\n",
            "Epoch 84/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.3306 - accuracy: 0.7713\n",
            "Epoch 85/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.3111 - accuracy: 0.7786\n",
            "Epoch 86/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.3020 - accuracy: 0.7776\n",
            "Epoch 87/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.2827 - accuracy: 0.7792\n",
            "Epoch 88/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2662 - accuracy: 0.7818\n",
            "Epoch 89/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2646 - accuracy: 0.7843\n",
            "Epoch 90/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2672 - accuracy: 0.7776\n",
            "Epoch 91/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.2424 - accuracy: 0.7869\n",
            "Epoch 92/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.2275 - accuracy: 0.7915\n",
            "Epoch 93/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.2189 - accuracy: 0.7883\n",
            "Epoch 94/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.2024 - accuracy: 0.7917\n",
            "Epoch 95/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1886 - accuracy: 0.7936\n",
            "Epoch 96/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1861 - accuracy: 0.7965\n",
            "Epoch 97/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1760 - accuracy: 0.7949\n",
            "Epoch 98/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1673 - accuracy: 0.7967\n",
            "Epoch 99/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1541 - accuracy: 0.8000\n",
            "Epoch 100/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1480 - accuracy: 0.7994\n",
            "Epoch 101/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1448 - accuracy: 0.8005\n",
            "Epoch 102/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1333 - accuracy: 0.8027\n",
            "Epoch 103/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.0993 - accuracy: 0.8105\n",
            "Epoch 104/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1009 - accuracy: 0.8085\n",
            "Epoch 105/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.1120 - accuracy: 0.8025\n",
            "Epoch 106/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1092 - accuracy: 0.8044\n",
            "Epoch 107/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0877 - accuracy: 0.8092\n",
            "Epoch 108/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0848 - accuracy: 0.8084\n",
            "Epoch 109/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0738 - accuracy: 0.8134\n",
            "Epoch 110/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0648 - accuracy: 0.8135\n",
            "Epoch 111/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0604 - accuracy: 0.8130\n",
            "Epoch 112/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0472 - accuracy: 0.8161\n",
            "Epoch 113/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0564 - accuracy: 0.8122\n",
            "Epoch 114/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0401 - accuracy: 0.8168\n",
            "Epoch 115/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0315 - accuracy: 0.8198\n",
            "Epoch 116/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0182 - accuracy: 0.8196\n",
            "Epoch 117/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0142 - accuracy: 0.8205\n",
            "Epoch 118/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0123 - accuracy: 0.8176\n",
            "Epoch 119/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0024 - accuracy: 0.8243\n",
            "Epoch 120/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9970 - accuracy: 0.8233\n",
            "Epoch 121/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9907 - accuracy: 0.8214\n",
            "Epoch 122/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9918 - accuracy: 0.8236\n",
            "Epoch 123/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9862 - accuracy: 0.8239\n",
            "Epoch 124/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9774 - accuracy: 0.8218\n",
            "Epoch 125/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9788 - accuracy: 0.8232\n",
            "Epoch 126/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9892 - accuracy: 0.8173\n",
            "Epoch 127/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9715 - accuracy: 0.8233\n",
            "Epoch 128/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9658 - accuracy: 0.8232\n",
            "Epoch 129/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9545 - accuracy: 0.8252\n",
            "Epoch 130/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9576 - accuracy: 0.8258\n",
            "Epoch 131/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9495 - accuracy: 0.8258\n",
            "Epoch 132/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9581 - accuracy: 0.8243\n",
            "Epoch 133/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9448 - accuracy: 0.8278\n",
            "Epoch 134/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 0.9533 - accuracy: 0.8209\n",
            "Epoch 135/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9433 - accuracy: 0.8254\n",
            "Epoch 136/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 0.9482 - accuracy: 0.8231\n",
            "Epoch 137/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9201 - accuracy: 0.8302\n",
            "Epoch 138/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9213 - accuracy: 0.8294\n",
            "Epoch 139/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 0.9120 - accuracy: 0.8328\n",
            "Epoch 140/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9233 - accuracy: 0.8266\n",
            "Epoch 141/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9205 - accuracy: 0.8284\n",
            "Epoch 142/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9079 - accuracy: 0.8300\n",
            "Epoch 143/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8981 - accuracy: 0.8324\n",
            "Epoch 144/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9001 - accuracy: 0.8311\n",
            "Epoch 145/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9049 - accuracy: 0.8284\n",
            "Epoch 146/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 0.8994 - accuracy: 0.8311\n",
            "Epoch 147/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8997 - accuracy: 0.8286\n",
            "Epoch 148/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8989 - accuracy: 0.8304\n",
            "Epoch 149/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8893 - accuracy: 0.8330\n",
            "Epoch 150/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8881 - accuracy: 0.8318\n",
            "Epoch 151/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8732 - accuracy: 0.8353\n",
            "Epoch 152/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8910 - accuracy: 0.8307\n",
            "Epoch 153/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8865 - accuracy: 0.8291\n",
            "Epoch 154/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8789 - accuracy: 0.8337\n",
            "Epoch 155/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8770 - accuracy: 0.8309\n",
            "Epoch 156/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8742 - accuracy: 0.8322\n",
            "Epoch 157/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8746 - accuracy: 0.8331\n",
            "Epoch 158/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8679 - accuracy: 0.8332\n",
            "Epoch 159/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8652 - accuracy: 0.8342\n",
            "Epoch 160/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8562 - accuracy: 0.8342\n",
            "Epoch 161/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8547 - accuracy: 0.8346\n",
            "Epoch 162/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8520 - accuracy: 0.8354\n",
            "Epoch 163/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8635 - accuracy: 0.8322\n",
            "Epoch 164/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8582 - accuracy: 0.8354\n",
            "Epoch 165/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8608 - accuracy: 0.8320\n",
            "Epoch 166/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8498 - accuracy: 0.8364\n",
            "Epoch 167/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8449 - accuracy: 0.8373\n",
            "Epoch 168/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 0.8353 - accuracy: 0.8377\n",
            "Epoch 169/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8399 - accuracy: 0.8362\n",
            "Epoch 170/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8540 - accuracy: 0.8360\n",
            "Epoch 171/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8553 - accuracy: 0.8326\n",
            "Epoch 172/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8404 - accuracy: 0.8344\n",
            "Epoch 173/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8378 - accuracy: 0.8370\n",
            "Epoch 174/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8401 - accuracy: 0.8333\n",
            "Epoch 175/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8231 - accuracy: 0.8388\n",
            "Epoch 176/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8152 - accuracy: 0.8395\n",
            "Epoch 177/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8263 - accuracy: 0.8390\n",
            "Epoch 178/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8283 - accuracy: 0.8373\n",
            "Epoch 179/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8370 - accuracy: 0.8338\n",
            "Epoch 180/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 0.8467 - accuracy: 0.8313\n",
            "Epoch 181/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8426 - accuracy: 0.8318\n",
            "Epoch 182/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8361 - accuracy: 0.8339\n",
            "Epoch 183/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8160 - accuracy: 0.8370\n",
            "Epoch 184/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8196 - accuracy: 0.8382\n",
            "Epoch 185/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8064 - accuracy: 0.8397\n",
            "Epoch 186/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8089 - accuracy: 0.8395\n",
            "Epoch 187/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8043 - accuracy: 0.8397\n",
            "Epoch 188/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8143 - accuracy: 0.8366\n",
            "Epoch 189/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8129 - accuracy: 0.8379\n",
            "Epoch 190/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8108 - accuracy: 0.8360\n",
            "Epoch 191/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8156 - accuracy: 0.8355\n",
            "Epoch 192/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8189 - accuracy: 0.8341\n",
            "Epoch 193/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8173 - accuracy: 0.8352\n",
            "Epoch 194/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8027 - accuracy: 0.8393\n",
            "Epoch 195/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7975 - accuracy: 0.8386\n",
            "Epoch 196/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8171 - accuracy: 0.8320\n",
            "Epoch 197/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8268 - accuracy: 0.8328\n",
            "Epoch 198/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8138 - accuracy: 0.8342\n",
            "Epoch 199/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7992 - accuracy: 0.8384\n",
            "Epoch 200/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8017 - accuracy: 0.8366\n",
            "Epoch 201/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7900 - accuracy: 0.8381\n",
            "Epoch 202/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7901 - accuracy: 0.8402\n",
            "Epoch 203/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7889 - accuracy: 0.8397\n",
            "Epoch 204/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7873 - accuracy: 0.8406\n",
            "Epoch 205/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7805 - accuracy: 0.8398\n",
            "Epoch 206/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8014 - accuracy: 0.8358\n",
            "Epoch 207/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7977 - accuracy: 0.8368\n",
            "Epoch 208/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7980 - accuracy: 0.8368\n",
            "Epoch 209/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7848 - accuracy: 0.8397\n",
            "Epoch 210/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7921 - accuracy: 0.8364\n",
            "Epoch 211/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7858 - accuracy: 0.8392\n",
            "Epoch 212/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7816 - accuracy: 0.8397\n",
            "Epoch 213/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7875 - accuracy: 0.8381\n",
            "Epoch 214/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7910 - accuracy: 0.8370\n",
            "Epoch 215/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7893 - accuracy: 0.8381\n",
            "Epoch 216/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7780 - accuracy: 0.8406\n",
            "Epoch 217/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7949 - accuracy: 0.8354\n",
            "Epoch 218/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7849 - accuracy: 0.8382\n",
            "Epoch 219/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7968 - accuracy: 0.8355\n",
            "Epoch 220/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7829 - accuracy: 0.8381\n",
            "Epoch 221/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7710 - accuracy: 0.8408\n",
            "Epoch 222/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7673 - accuracy: 0.8392\n",
            "Epoch 223/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7730 - accuracy: 0.8404\n",
            "Epoch 224/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7717 - accuracy: 0.8388\n",
            "Epoch 225/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7849 - accuracy: 0.8358\n",
            "Epoch 226/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7646 - accuracy: 0.8417\n",
            "Epoch 227/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7856 - accuracy: 0.8364\n",
            "Epoch 228/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7692 - accuracy: 0.8397\n",
            "Epoch 229/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7674 - accuracy: 0.8400\n",
            "Epoch 230/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7564 - accuracy: 0.8415\n",
            "Epoch 231/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7658 - accuracy: 0.8405\n",
            "Epoch 232/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7781 - accuracy: 0.8383\n",
            "Epoch 233/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7700 - accuracy: 0.8386\n",
            "Epoch 234/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7807 - accuracy: 0.8371\n",
            "Epoch 235/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7798 - accuracy: 0.8368\n",
            "Epoch 236/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7656 - accuracy: 0.8394\n",
            "Epoch 237/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7633 - accuracy: 0.8405\n",
            "Epoch 238/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7508 - accuracy: 0.8427\n",
            "Epoch 239/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7653 - accuracy: 0.8381\n",
            "Epoch 240/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7768 - accuracy: 0.8371\n",
            "Epoch 241/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7709 - accuracy: 0.8377\n",
            "Epoch 242/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7547 - accuracy: 0.8406\n",
            "Epoch 243/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7568 - accuracy: 0.8408\n",
            "Epoch 244/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7516 - accuracy: 0.8390\n",
            "Epoch 245/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7636 - accuracy: 0.8396\n",
            "Epoch 246/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7548 - accuracy: 0.8393\n",
            "Epoch 247/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7593 - accuracy: 0.8407\n",
            "Epoch 248/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7621 - accuracy: 0.8377\n",
            "Epoch 249/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7533 - accuracy: 0.8384\n",
            "Epoch 250/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7520 - accuracy: 0.8402\n"
          ]
        }
      ],
      "source": [
        "# Get the untrained model\n",
        "model = create_model(total_words, max_sequence_len)\n",
        "model.summary()\n",
        "# Train the model\n",
        "history = model.fit(features, labels, epochs=250, verbose=1, callbacks=[callbacks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "1fXTEO3GJ282",
        "outputId": "397d0ba8-7c2b-4c73-c1b8-2b8d4b73cfc3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Take a look at the training curves of your model\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.title('Training accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "plt.title('Training loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = create_model1(total_words, max_sequence_len)\n",
        "model1.summary()\n",
        "# Train the model\n",
        "history = model1.fit(features, labels, epochs=250, verbose=1, callbacks=[callbacks])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHZnNDyHmONV",
        "outputId": "0e09fb61-74b2-4a21-b034-59f156f96b14"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 10, 100)           321100    \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 10, 350)          386400    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 10, 350)           0         \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 125)               238000    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1605)              202230    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 3211)              5156866   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,304,596\n",
            "Trainable params: 6,304,596\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 10, 100)           321100    \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 10, 350)          386400    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 10, 350)           0         \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 125)               238000    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1605)              202230    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 3211)              5156866   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,304,596\n",
            "Trainable params: 6,304,596\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/250\n",
            "484/484 [==============================] - 8s 9ms/step - loss: 6.9272 - accuracy: 0.0222\n",
            "Epoch 2/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 6.5027 - accuracy: 0.0219\n",
            "Epoch 3/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 6.4123 - accuracy: 0.0244\n",
            "Epoch 4/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 6.3042 - accuracy: 0.0299\n",
            "Epoch 5/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 6.2113 - accuracy: 0.0338\n",
            "Epoch 6/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 6.1431 - accuracy: 0.0366\n",
            "Epoch 7/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 6.0813 - accuracy: 0.0393\n",
            "Epoch 8/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 6.0199 - accuracy: 0.0428\n",
            "Epoch 9/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 5.9464 - accuracy: 0.0469\n",
            "Epoch 10/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 5.8633 - accuracy: 0.0511\n",
            "Epoch 11/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 5.7677 - accuracy: 0.0568\n",
            "Epoch 12/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 5.6758 - accuracy: 0.0629\n",
            "Epoch 13/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 5.5743 - accuracy: 0.0677\n",
            "Epoch 14/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 5.4773 - accuracy: 0.0739\n",
            "Epoch 15/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 5.3787 - accuracy: 0.0786\n",
            "Epoch 16/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 5.2840 - accuracy: 0.0856\n",
            "Epoch 17/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 5.1863 - accuracy: 0.0885\n",
            "Epoch 18/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 5.0825 - accuracy: 0.0969\n",
            "Epoch 19/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 4.9799 - accuracy: 0.1037\n",
            "Epoch 20/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 4.8794 - accuracy: 0.1107\n",
            "Epoch 21/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 4.7687 - accuracy: 0.1218\n",
            "Epoch 22/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 4.6625 - accuracy: 0.1299\n",
            "Epoch 23/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 4.5542 - accuracy: 0.1398\n",
            "Epoch 24/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 4.4427 - accuracy: 0.1496\n",
            "Epoch 25/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 4.3378 - accuracy: 0.1659\n",
            "Epoch 26/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 4.2278 - accuracy: 0.1750\n",
            "Epoch 27/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 4.1100 - accuracy: 0.1854\n",
            "Epoch 28/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 4.0058 - accuracy: 0.2034\n",
            "Epoch 29/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.8890 - accuracy: 0.2235\n",
            "Epoch 30/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.7973 - accuracy: 0.2375\n",
            "Epoch 31/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.6952 - accuracy: 0.2596\n",
            "Epoch 32/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.5957 - accuracy: 0.2820\n",
            "Epoch 33/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.4966 - accuracy: 0.2978\n",
            "Epoch 34/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.3995 - accuracy: 0.3212\n",
            "Epoch 35/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.3042 - accuracy: 0.3451\n",
            "Epoch 36/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.2159 - accuracy: 0.3644\n",
            "Epoch 37/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.1301 - accuracy: 0.3847\n",
            "Epoch 38/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 3.0564 - accuracy: 0.4033\n",
            "Epoch 39/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.9750 - accuracy: 0.4167\n",
            "Epoch 40/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.8925 - accuracy: 0.4393\n",
            "Epoch 41/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.8201 - accuracy: 0.4546\n",
            "Epoch 42/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.7430 - accuracy: 0.4708\n",
            "Epoch 43/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.6730 - accuracy: 0.4875\n",
            "Epoch 44/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.6077 - accuracy: 0.5043\n",
            "Epoch 45/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.5467 - accuracy: 0.5187\n",
            "Epoch 46/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.4868 - accuracy: 0.5324\n",
            "Epoch 47/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.4273 - accuracy: 0.5464\n",
            "Epoch 48/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.3700 - accuracy: 0.5589\n",
            "Epoch 49/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.3025 - accuracy: 0.5783\n",
            "Epoch 50/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.2601 - accuracy: 0.5825\n",
            "Epoch 51/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.2000 - accuracy: 0.5983\n",
            "Epoch 52/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.1689 - accuracy: 0.6044\n",
            "Epoch 53/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.1096 - accuracy: 0.6180\n",
            "Epoch 54/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.0764 - accuracy: 0.6263\n",
            "Epoch 55/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.0320 - accuracy: 0.6372\n",
            "Epoch 56/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.9748 - accuracy: 0.6515\n",
            "Epoch 57/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.9389 - accuracy: 0.6581\n",
            "Epoch 58/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.9023 - accuracy: 0.6639\n",
            "Epoch 59/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.8731 - accuracy: 0.6702\n",
            "Epoch 60/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.8297 - accuracy: 0.6802\n",
            "Epoch 61/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.8025 - accuracy: 0.6868\n",
            "Epoch 62/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.7730 - accuracy: 0.6922\n",
            "Epoch 63/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.7298 - accuracy: 0.7035\n",
            "Epoch 64/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.7089 - accuracy: 0.7093\n",
            "Epoch 65/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.6724 - accuracy: 0.7115\n",
            "Epoch 66/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.6406 - accuracy: 0.7174\n",
            "Epoch 67/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.6175 - accuracy: 0.7269\n",
            "Epoch 68/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.5839 - accuracy: 0.7308\n",
            "Epoch 69/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.5695 - accuracy: 0.7327\n",
            "Epoch 70/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.5650 - accuracy: 0.7339\n",
            "Epoch 71/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.5164 - accuracy: 0.7427\n",
            "Epoch 72/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.4908 - accuracy: 0.7515\n",
            "Epoch 73/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.4610 - accuracy: 0.7571\n",
            "Epoch 74/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.4433 - accuracy: 0.7577\n",
            "Epoch 75/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.4222 - accuracy: 0.7630\n",
            "Epoch 76/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.4098 - accuracy: 0.7629\n",
            "Epoch 77/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.3976 - accuracy: 0.7646\n",
            "Epoch 78/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.3692 - accuracy: 0.7720\n",
            "Epoch 79/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.3460 - accuracy: 0.7764\n",
            "Epoch 80/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.3290 - accuracy: 0.7787\n",
            "Epoch 81/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.3247 - accuracy: 0.7806\n",
            "Epoch 82/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.3028 - accuracy: 0.7831\n",
            "Epoch 83/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.2853 - accuracy: 0.7870\n",
            "Epoch 84/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.2672 - accuracy: 0.7891\n",
            "Epoch 85/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.2743 - accuracy: 0.7849\n",
            "Epoch 86/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.2655 - accuracy: 0.7874\n",
            "Epoch 87/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.2400 - accuracy: 0.7930\n",
            "Epoch 88/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.2053 - accuracy: 0.7994\n",
            "Epoch 89/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1966 - accuracy: 0.8013\n",
            "Epoch 90/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1952 - accuracy: 0.7988\n",
            "Epoch 91/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1850 - accuracy: 0.7992\n",
            "Epoch 92/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1664 - accuracy: 0.8040\n",
            "Epoch 93/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1556 - accuracy: 0.8035\n",
            "Epoch 94/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1598 - accuracy: 0.8062\n",
            "Epoch 95/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.1430 - accuracy: 0.8089\n",
            "Epoch 96/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1346 - accuracy: 0.8084\n",
            "Epoch 97/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1152 - accuracy: 0.8119\n",
            "Epoch 98/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1019 - accuracy: 0.8148\n",
            "Epoch 99/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0957 - accuracy: 0.8176\n",
            "Epoch 100/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1029 - accuracy: 0.8125\n",
            "Epoch 101/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0870 - accuracy: 0.8144\n",
            "Epoch 102/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0814 - accuracy: 0.8159\n",
            "Epoch 103/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0795 - accuracy: 0.8159\n",
            "Epoch 104/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0637 - accuracy: 0.8196\n",
            "Epoch 105/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0462 - accuracy: 0.8199\n",
            "Epoch 106/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.0405 - accuracy: 0.8209\n",
            "Epoch 107/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0395 - accuracy: 0.8217\n",
            "Epoch 108/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0283 - accuracy: 0.8246\n",
            "Epoch 109/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0285 - accuracy: 0.8247\n",
            "Epoch 110/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0184 - accuracy: 0.8246\n",
            "Epoch 111/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0070 - accuracy: 0.8272\n",
            "Epoch 112/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0097 - accuracy: 0.8246\n",
            "Epoch 113/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0049 - accuracy: 0.8259\n",
            "Epoch 114/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9924 - accuracy: 0.8260\n",
            "Epoch 115/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9814 - accuracy: 0.8291\n",
            "Epoch 116/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9760 - accuracy: 0.8265\n",
            "Epoch 117/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9862 - accuracy: 0.8273\n",
            "Epoch 118/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9784 - accuracy: 0.8263\n",
            "Epoch 119/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9741 - accuracy: 0.8249\n",
            "Epoch 120/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9706 - accuracy: 0.8284\n",
            "Epoch 121/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9530 - accuracy: 0.8299\n",
            "Epoch 122/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9520 - accuracy: 0.8298\n",
            "Epoch 123/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9693 - accuracy: 0.8269\n",
            "Epoch 124/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9399 - accuracy: 0.8321\n",
            "Epoch 125/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9418 - accuracy: 0.8306\n",
            "Epoch 126/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9347 - accuracy: 0.8320\n",
            "Epoch 127/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9224 - accuracy: 0.8328\n",
            "Epoch 128/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9262 - accuracy: 0.8320\n",
            "Epoch 129/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9231 - accuracy: 0.8322\n",
            "Epoch 130/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9176 - accuracy: 0.8331\n",
            "Epoch 131/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9289 - accuracy: 0.8343\n",
            "Epoch 132/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9198 - accuracy: 0.8352\n",
            "Epoch 133/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8995 - accuracy: 0.8373\n",
            "Epoch 134/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9039 - accuracy: 0.8355\n",
            "Epoch 135/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8976 - accuracy: 0.8371\n",
            "Epoch 136/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8865 - accuracy: 0.8373\n",
            "Epoch 137/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8979 - accuracy: 0.8357\n",
            "Epoch 138/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8990 - accuracy: 0.8338\n",
            "Epoch 139/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8942 - accuracy: 0.8372\n",
            "Epoch 140/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8819 - accuracy: 0.8377\n",
            "Epoch 141/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8671 - accuracy: 0.8403\n",
            "Epoch 142/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8791 - accuracy: 0.8361\n",
            "Epoch 143/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8776 - accuracy: 0.8357\n",
            "Epoch 144/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8779 - accuracy: 0.8362\n",
            "Epoch 145/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8863 - accuracy: 0.8351\n",
            "Epoch 146/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8803 - accuracy: 0.8357\n",
            "Epoch 147/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8796 - accuracy: 0.8330\n",
            "Epoch 148/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8662 - accuracy: 0.8369\n",
            "Epoch 149/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8551 - accuracy: 0.8377\n",
            "Epoch 150/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8484 - accuracy: 0.8417\n",
            "Epoch 151/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8567 - accuracy: 0.8380\n",
            "Epoch 152/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8455 - accuracy: 0.8408\n",
            "Epoch 153/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8472 - accuracy: 0.8378\n",
            "Epoch 154/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8498 - accuracy: 0.8375\n",
            "Epoch 155/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8634 - accuracy: 0.8355\n",
            "Epoch 156/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8577 - accuracy: 0.8364\n",
            "Epoch 157/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8432 - accuracy: 0.8379\n",
            "Epoch 158/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8509 - accuracy: 0.8374\n",
            "Epoch 159/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8437 - accuracy: 0.8381\n",
            "Epoch 160/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8397 - accuracy: 0.8382\n",
            "Epoch 161/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8346 - accuracy: 0.8386\n",
            "Epoch 162/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8295 - accuracy: 0.8391\n",
            "Epoch 163/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8295 - accuracy: 0.8414\n",
            "Epoch 164/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 0.8291 - accuracy: 0.8386\n",
            "Epoch 165/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8296 - accuracy: 0.8380\n",
            "Epoch 166/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8239 - accuracy: 0.8414\n",
            "Epoch 167/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8139 - accuracy: 0.8426\n",
            "Epoch 168/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8101 - accuracy: 0.8414\n",
            "Epoch 169/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8176 - accuracy: 0.8413\n",
            "Epoch 170/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8221 - accuracy: 0.8397\n",
            "Epoch 171/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8150 - accuracy: 0.8399\n",
            "Epoch 172/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8202 - accuracy: 0.8410\n",
            "Epoch 173/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8156 - accuracy: 0.8394\n",
            "Epoch 174/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8176 - accuracy: 0.8373\n",
            "Epoch 175/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8121 - accuracy: 0.8397\n",
            "Epoch 176/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8128 - accuracy: 0.8408\n",
            "Epoch 177/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8109 - accuracy: 0.8401\n",
            "Epoch 178/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8068 - accuracy: 0.8401\n",
            "Epoch 179/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7979 - accuracy: 0.8428\n",
            "Epoch 180/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7973 - accuracy: 0.8436\n",
            "Epoch 181/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8015 - accuracy: 0.8414\n",
            "Epoch 182/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8082 - accuracy: 0.8396\n",
            "Epoch 183/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8050 - accuracy: 0.8374\n",
            "Epoch 184/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8027 - accuracy: 0.8397\n",
            "Epoch 185/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7942 - accuracy: 0.8421\n",
            "Epoch 186/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7952 - accuracy: 0.8410\n",
            "Epoch 187/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7994 - accuracy: 0.8388\n",
            "Epoch 188/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8075 - accuracy: 0.8372\n",
            "Epoch 189/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8056 - accuracy: 0.8381\n",
            "Epoch 190/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7896 - accuracy: 0.8439\n",
            "Epoch 191/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7777 - accuracy: 0.8439\n",
            "Epoch 192/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7786 - accuracy: 0.8439\n",
            "Epoch 193/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7779 - accuracy: 0.8431\n",
            "Epoch 194/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7927 - accuracy: 0.8410\n",
            "Epoch 195/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7941 - accuracy: 0.8414\n",
            "Epoch 196/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7957 - accuracy: 0.8397\n",
            "Epoch 197/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7844 - accuracy: 0.8421\n",
            "Epoch 198/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7777 - accuracy: 0.8429\n",
            "Epoch 199/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7936 - accuracy: 0.8378\n",
            "Epoch 200/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7830 - accuracy: 0.8417\n",
            "Epoch 201/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7712 - accuracy: 0.8436\n",
            "Epoch 202/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7792 - accuracy: 0.8399\n",
            "Epoch 203/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7843 - accuracy: 0.8390\n",
            "Epoch 204/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7780 - accuracy: 0.8407\n",
            "Epoch 205/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7782 - accuracy: 0.8420\n",
            "Epoch 206/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7692 - accuracy: 0.8441\n",
            "Epoch 207/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7685 - accuracy: 0.8437\n",
            "Epoch 208/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7706 - accuracy: 0.8430\n",
            "Epoch 209/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7763 - accuracy: 0.8410\n",
            "Epoch 210/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7821 - accuracy: 0.8403\n",
            "Epoch 211/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7726 - accuracy: 0.8411\n",
            "Epoch 212/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7807 - accuracy: 0.8384\n",
            "Epoch 213/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7669 - accuracy: 0.8420\n",
            "Epoch 214/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7480 - accuracy: 0.8451\n",
            "Epoch 215/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7570 - accuracy: 0.8439\n",
            "Epoch 216/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7684 - accuracy: 0.8402\n",
            "Epoch 217/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7760 - accuracy: 0.8392\n",
            "Epoch 218/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7617 - accuracy: 0.8427\n",
            "Epoch 219/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7586 - accuracy: 0.8420\n",
            "Epoch 220/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7572 - accuracy: 0.8424\n",
            "Epoch 221/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7615 - accuracy: 0.8416\n",
            "Epoch 222/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7555 - accuracy: 0.8428\n",
            "Epoch 223/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7598 - accuracy: 0.8421\n",
            "Epoch 224/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7737 - accuracy: 0.8404\n",
            "Epoch 225/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7654 - accuracy: 0.8416\n",
            "Epoch 226/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7511 - accuracy: 0.8445\n",
            "Epoch 227/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7474 - accuracy: 0.8436\n",
            "Epoch 228/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7541 - accuracy: 0.8434\n",
            "Epoch 229/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7626 - accuracy: 0.8410\n",
            "Epoch 230/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7527 - accuracy: 0.8421\n",
            "Epoch 231/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7616 - accuracy: 0.8393\n",
            "Epoch 232/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7498 - accuracy: 0.8432\n",
            "Epoch 233/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7474 - accuracy: 0.8439\n",
            "Epoch 234/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7496 - accuracy: 0.8429\n",
            "Epoch 235/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7541 - accuracy: 0.8409\n",
            "Epoch 236/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7551 - accuracy: 0.8411\n",
            "Epoch 237/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7467 - accuracy: 0.8423\n",
            "Epoch 238/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7506 - accuracy: 0.8422\n",
            "Epoch 239/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7528 - accuracy: 0.8404\n",
            "Epoch 240/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7540 - accuracy: 0.8414\n",
            "Epoch 241/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7518 - accuracy: 0.8434\n",
            "Epoch 242/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7529 - accuracy: 0.8419\n",
            "Epoch 243/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7383 - accuracy: 0.8437\n",
            "Epoch 244/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.7354 - accuracy: 0.8446\n",
            "Epoch 245/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7381 - accuracy: 0.8451\n",
            "Epoch 246/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7319 - accuracy: 0.8458\n",
            "Epoch 247/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7503 - accuracy: 0.8388\n",
            "Epoch 248/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7449 - accuracy: 0.8423\n",
            "Epoch 249/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7527 - accuracy: 0.8403\n",
            "Epoch 250/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7504 - accuracy: 0.8412\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the training curves of your model\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.title('Training accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "plt.title('Training loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "nvsxlfWPmUhY",
        "outputId": "bc5c8d0a-80ff-4992-dc95-26d95a2c178d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = create_model2(total_words, max_sequence_len)\n",
        "model2.summary()\n",
        "# Train the model\n",
        "history = model2.fit(features, labels, epochs=250, verbose=1, callbacks=[callbacks])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJ-iipwPmYH7",
        "outputId": "45265662-7071-42b3-8a0a-527f881e077f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 10, 100)           321100    \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 10, 300)          301200    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 10, 300)           0         \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1605)              162105    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 3211)              5156866   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,101,671\n",
            "Trainable params: 6,101,671\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 10, 100)           321100    \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 10, 300)          301200    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 10, 300)           0         \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1605)              162105    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 3211)              5156866   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,101,671\n",
            "Trainable params: 6,101,671\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/250\n",
            "484/484 [==============================] - 8s 9ms/step - loss: 6.9124 - accuracy: 0.0220\n",
            "Epoch 2/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 6.5008 - accuracy: 0.0230\n",
            "Epoch 3/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 6.4051 - accuracy: 0.0263\n",
            "Epoch 4/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 6.2979 - accuracy: 0.0279\n",
            "Epoch 5/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 6.2165 - accuracy: 0.0343\n",
            "Epoch 6/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 6.1501 - accuracy: 0.0373\n",
            "Epoch 7/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 6.0818 - accuracy: 0.0386\n",
            "Epoch 8/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 6.0105 - accuracy: 0.0420\n",
            "Epoch 9/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 5.9247 - accuracy: 0.0462\n",
            "Epoch 10/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 5.8193 - accuracy: 0.0518\n",
            "Epoch 11/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 5.7093 - accuracy: 0.0589\n",
            "Epoch 12/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 5.5901 - accuracy: 0.0636\n",
            "Epoch 13/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 5.4810 - accuracy: 0.0711\n",
            "Epoch 14/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 5.3783 - accuracy: 0.0757\n",
            "Epoch 15/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 5.2750 - accuracy: 0.0788\n",
            "Epoch 16/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 5.1764 - accuracy: 0.0865\n",
            "Epoch 17/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 5.0806 - accuracy: 0.0905\n",
            "Epoch 18/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 4.9893 - accuracy: 0.0964\n",
            "Epoch 19/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 4.8931 - accuracy: 0.1035\n",
            "Epoch 20/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 4.7965 - accuracy: 0.1106\n",
            "Epoch 21/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 4.6969 - accuracy: 0.1217\n",
            "Epoch 22/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 4.5985 - accuracy: 0.1270\n",
            "Epoch 23/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 4.4950 - accuracy: 0.1378\n",
            "Epoch 24/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 4.3938 - accuracy: 0.1477\n",
            "Epoch 25/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 4.3003 - accuracy: 0.1591\n",
            "Epoch 26/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 4.2007 - accuracy: 0.1688\n",
            "Epoch 27/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 4.1055 - accuracy: 0.1806\n",
            "Epoch 28/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 4.0189 - accuracy: 0.1956\n",
            "Epoch 29/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.9168 - accuracy: 0.2144\n",
            "Epoch 30/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.8312 - accuracy: 0.2294\n",
            "Epoch 31/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.7387 - accuracy: 0.2473\n",
            "Epoch 32/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.6638 - accuracy: 0.2689\n",
            "Epoch 33/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.5662 - accuracy: 0.2842\n",
            "Epoch 34/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.4959 - accuracy: 0.2931\n",
            "Epoch 35/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.4147 - accuracy: 0.3186\n",
            "Epoch 36/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.3407 - accuracy: 0.3296\n",
            "Epoch 37/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.2840 - accuracy: 0.3441\n",
            "Epoch 38/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.2007 - accuracy: 0.3597\n",
            "Epoch 39/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.1300 - accuracy: 0.3721\n",
            "Epoch 40/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.0731 - accuracy: 0.3837\n",
            "Epoch 41/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.0093 - accuracy: 0.4040\n",
            "Epoch 42/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.9459 - accuracy: 0.4139\n",
            "Epoch 43/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.8817 - accuracy: 0.4278\n",
            "Epoch 44/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.8279 - accuracy: 0.4428\n",
            "Epoch 45/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.7737 - accuracy: 0.4543\n",
            "Epoch 46/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.7365 - accuracy: 0.4616\n",
            "Epoch 47/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.6788 - accuracy: 0.4767\n",
            "Epoch 48/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.6199 - accuracy: 0.4894\n",
            "Epoch 49/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.5757 - accuracy: 0.4945\n",
            "Epoch 50/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.5263 - accuracy: 0.5056\n",
            "Epoch 51/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.4858 - accuracy: 0.5191\n",
            "Epoch 52/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.4399 - accuracy: 0.5304\n",
            "Epoch 53/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.3975 - accuracy: 0.5369\n",
            "Epoch 54/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.3498 - accuracy: 0.5457\n",
            "Epoch 55/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.3178 - accuracy: 0.5541\n",
            "Epoch 56/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.2708 - accuracy: 0.5672\n",
            "Epoch 57/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.2331 - accuracy: 0.5768\n",
            "Epoch 58/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.1981 - accuracy: 0.5791\n",
            "Epoch 59/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.1615 - accuracy: 0.5902\n",
            "Epoch 60/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.1200 - accuracy: 0.5970\n",
            "Epoch 61/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.0898 - accuracy: 0.6030\n",
            "Epoch 62/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.0657 - accuracy: 0.6069\n",
            "Epoch 63/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.0382 - accuracy: 0.6149\n",
            "Epoch 64/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.9936 - accuracy: 0.6254\n",
            "Epoch 65/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.9729 - accuracy: 0.6301\n",
            "Epoch 66/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.9360 - accuracy: 0.6398\n",
            "Epoch 67/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.9038 - accuracy: 0.6486\n",
            "Epoch 68/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.8809 - accuracy: 0.6496\n",
            "Epoch 69/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.8555 - accuracy: 0.6524\n",
            "Epoch 70/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.8236 - accuracy: 0.6654\n",
            "Epoch 71/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.7999 - accuracy: 0.6684\n",
            "Epoch 72/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.8009 - accuracy: 0.6664\n",
            "Epoch 73/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.7493 - accuracy: 0.6785\n",
            "Epoch 74/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.7320 - accuracy: 0.6815\n",
            "Epoch 75/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.7164 - accuracy: 0.6841\n",
            "Epoch 76/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.6859 - accuracy: 0.6925\n",
            "Epoch 77/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.6615 - accuracy: 0.6923\n",
            "Epoch 78/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.6471 - accuracy: 0.6996\n",
            "Epoch 79/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.6288 - accuracy: 0.7006\n",
            "Epoch 80/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.6035 - accuracy: 0.7051\n",
            "Epoch 81/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.5924 - accuracy: 0.7082\n",
            "Epoch 82/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.5717 - accuracy: 0.7111\n",
            "Epoch 83/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.5551 - accuracy: 0.7198\n",
            "Epoch 84/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.5214 - accuracy: 0.7265\n",
            "Epoch 85/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.5183 - accuracy: 0.7269\n",
            "Epoch 86/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.5024 - accuracy: 0.7296\n",
            "Epoch 87/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.4959 - accuracy: 0.7302\n",
            "Epoch 88/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4679 - accuracy: 0.7368\n",
            "Epoch 89/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4449 - accuracy: 0.7411\n",
            "Epoch 90/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4540 - accuracy: 0.7359\n",
            "Epoch 91/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.4337 - accuracy: 0.7392\n",
            "Epoch 92/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.4038 - accuracy: 0.7462\n",
            "Epoch 93/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.4032 - accuracy: 0.7485\n",
            "Epoch 94/250\n",
            "484/484 [==============================] - 8s 16ms/step - loss: 1.3821 - accuracy: 0.7527\n",
            "Epoch 95/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.3737 - accuracy: 0.7515\n",
            "Epoch 96/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3555 - accuracy: 0.7552\n",
            "Epoch 97/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.3495 - accuracy: 0.7569\n",
            "Epoch 98/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.3314 - accuracy: 0.7627\n",
            "Epoch 99/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.3172 - accuracy: 0.7630\n",
            "Epoch 100/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.3124 - accuracy: 0.7642\n",
            "Epoch 101/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.3092 - accuracy: 0.7643\n",
            "Epoch 102/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.2924 - accuracy: 0.7688\n",
            "Epoch 103/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.2700 - accuracy: 0.7713\n",
            "Epoch 104/250\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 1.2636 - accuracy: 0.7746\n",
            "Epoch 105/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.2506 - accuracy: 0.7759\n",
            "Epoch 106/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2502 - accuracy: 0.7738\n",
            "Epoch 107/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2415 - accuracy: 0.7776\n",
            "Epoch 108/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.2366 - accuracy: 0.7765\n",
            "Epoch 109/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.2197 - accuracy: 0.7824\n",
            "Epoch 110/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.2111 - accuracy: 0.7837\n",
            "Epoch 111/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.2037 - accuracy: 0.7838\n",
            "Epoch 112/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1993 - accuracy: 0.7844\n",
            "Epoch 113/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1820 - accuracy: 0.7886\n",
            "Epoch 114/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1674 - accuracy: 0.7925\n",
            "Epoch 115/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1820 - accuracy: 0.7853\n",
            "Epoch 116/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1878 - accuracy: 0.7830\n",
            "Epoch 117/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1729 - accuracy: 0.7875\n",
            "Epoch 118/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1447 - accuracy: 0.7911\n",
            "Epoch 119/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1427 - accuracy: 0.7907\n",
            "Epoch 120/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1385 - accuracy: 0.7947\n",
            "Epoch 121/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1283 - accuracy: 0.7955\n",
            "Epoch 122/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1127 - accuracy: 0.7980\n",
            "Epoch 123/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1064 - accuracy: 0.8009\n",
            "Epoch 124/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1066 - accuracy: 0.7992\n",
            "Epoch 125/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1125 - accuracy: 0.7985\n",
            "Epoch 126/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1120 - accuracy: 0.7972\n",
            "Epoch 127/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0878 - accuracy: 0.8031\n",
            "Epoch 128/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0913 - accuracy: 0.8011\n",
            "Epoch 129/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0843 - accuracy: 0.8033\n",
            "Epoch 130/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0806 - accuracy: 0.8026\n",
            "Epoch 131/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0758 - accuracy: 0.8037\n",
            "Epoch 132/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0730 - accuracy: 0.8042\n",
            "Epoch 133/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0657 - accuracy: 0.8044\n",
            "Epoch 134/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0498 - accuracy: 0.8087\n",
            "Epoch 135/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0468 - accuracy: 0.8080\n",
            "Epoch 136/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.0519 - accuracy: 0.8051\n",
            "Epoch 137/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0339 - accuracy: 0.8058\n",
            "Epoch 138/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0230 - accuracy: 0.8115\n",
            "Epoch 139/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0255 - accuracy: 0.8088\n",
            "Epoch 140/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0324 - accuracy: 0.8063\n",
            "Epoch 141/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0251 - accuracy: 0.8106\n",
            "Epoch 142/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0307 - accuracy: 0.8077\n",
            "Epoch 143/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0158 - accuracy: 0.8116\n",
            "Epoch 144/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0171 - accuracy: 0.8099\n",
            "Epoch 145/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0178 - accuracy: 0.8099\n",
            "Epoch 146/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0117 - accuracy: 0.8080\n",
            "Epoch 147/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0050 - accuracy: 0.8113\n",
            "Epoch 148/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9930 - accuracy: 0.8142\n",
            "Epoch 149/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9958 - accuracy: 0.8126\n",
            "Epoch 150/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9873 - accuracy: 0.8176\n",
            "Epoch 151/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9839 - accuracy: 0.8159\n",
            "Epoch 152/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9811 - accuracy: 0.8128\n",
            "Epoch 153/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9816 - accuracy: 0.8149\n",
            "Epoch 154/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9752 - accuracy: 0.8160\n",
            "Epoch 155/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9730 - accuracy: 0.8169\n",
            "Epoch 156/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9616 - accuracy: 0.8205\n",
            "Epoch 157/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 0.9578 - accuracy: 0.8204\n",
            "Epoch 158/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9624 - accuracy: 0.8160\n",
            "Epoch 159/250\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 0.9605 - accuracy: 0.8181\n",
            "Epoch 160/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9674 - accuracy: 0.8137\n",
            "Epoch 161/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9466 - accuracy: 0.8219\n",
            "Epoch 162/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9461 - accuracy: 0.8226\n",
            "Epoch 163/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9471 - accuracy: 0.8200\n",
            "Epoch 164/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9482 - accuracy: 0.8208\n",
            "Epoch 165/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9507 - accuracy: 0.8179\n",
            "Epoch 166/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9361 - accuracy: 0.8210\n",
            "Epoch 167/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9325 - accuracy: 0.8208\n",
            "Epoch 168/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9341 - accuracy: 0.8211\n",
            "Epoch 169/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9354 - accuracy: 0.8192\n",
            "Epoch 170/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9224 - accuracy: 0.8223\n",
            "Epoch 171/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9245 - accuracy: 0.8208\n",
            "Epoch 172/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9258 - accuracy: 0.8232\n",
            "Epoch 173/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9234 - accuracy: 0.8237\n",
            "Epoch 174/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9188 - accuracy: 0.8243\n",
            "Epoch 175/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9159 - accuracy: 0.8232\n",
            "Epoch 176/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9190 - accuracy: 0.8210\n",
            "Epoch 177/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9064 - accuracy: 0.8221\n",
            "Epoch 178/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9097 - accuracy: 0.8222\n",
            "Epoch 179/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8992 - accuracy: 0.8279\n",
            "Epoch 180/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9103 - accuracy: 0.8231\n",
            "Epoch 181/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9049 - accuracy: 0.8221\n",
            "Epoch 182/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8968 - accuracy: 0.8280\n",
            "Epoch 183/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8958 - accuracy: 0.8253\n",
            "Epoch 184/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9030 - accuracy: 0.8247\n",
            "Epoch 185/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9013 - accuracy: 0.8227\n",
            "Epoch 186/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8922 - accuracy: 0.8262\n",
            "Epoch 187/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8971 - accuracy: 0.8251\n",
            "Epoch 188/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8949 - accuracy: 0.8257\n",
            "Epoch 189/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8934 - accuracy: 0.8246\n",
            "Epoch 190/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8901 - accuracy: 0.8247\n",
            "Epoch 191/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8769 - accuracy: 0.8276\n",
            "Epoch 192/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8786 - accuracy: 0.8267\n",
            "Epoch 193/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8879 - accuracy: 0.8232\n",
            "Epoch 194/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8748 - accuracy: 0.8268\n",
            "Epoch 195/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8750 - accuracy: 0.8268\n",
            "Epoch 196/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8645 - accuracy: 0.8306\n",
            "Epoch 197/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8805 - accuracy: 0.8251\n",
            "Epoch 198/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8670 - accuracy: 0.8280\n",
            "Epoch 199/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8680 - accuracy: 0.8278\n",
            "Epoch 200/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8613 - accuracy: 0.8289\n",
            "Epoch 201/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8719 - accuracy: 0.8273\n",
            "Epoch 202/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8680 - accuracy: 0.8272\n",
            "Epoch 203/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8649 - accuracy: 0.8276\n",
            "Epoch 204/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8792 - accuracy: 0.8243\n",
            "Epoch 205/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8635 - accuracy: 0.8293\n",
            "Epoch 206/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8614 - accuracy: 0.8280\n",
            "Epoch 207/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8684 - accuracy: 0.8251\n",
            "Epoch 208/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8576 - accuracy: 0.8279\n",
            "Epoch 209/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8469 - accuracy: 0.8298\n",
            "Epoch 210/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8456 - accuracy: 0.8332\n",
            "Epoch 211/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8558 - accuracy: 0.8295\n",
            "Epoch 212/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8506 - accuracy: 0.8281\n",
            "Epoch 213/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8528 - accuracy: 0.8278\n",
            "Epoch 214/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8485 - accuracy: 0.8289\n",
            "Epoch 215/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8516 - accuracy: 0.8274\n",
            "Epoch 216/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8568 - accuracy: 0.8277\n",
            "Epoch 217/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8461 - accuracy: 0.8295\n",
            "Epoch 218/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8426 - accuracy: 0.8307\n",
            "Epoch 219/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8418 - accuracy: 0.8285\n",
            "Epoch 220/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8407 - accuracy: 0.8311\n",
            "Epoch 221/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8491 - accuracy: 0.8277\n",
            "Epoch 222/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8463 - accuracy: 0.8280\n",
            "Epoch 223/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8365 - accuracy: 0.8320\n",
            "Epoch 224/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8375 - accuracy: 0.8289\n",
            "Epoch 225/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8308 - accuracy: 0.8317\n",
            "Epoch 226/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8281 - accuracy: 0.8318\n",
            "Epoch 227/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8290 - accuracy: 0.8329\n",
            "Epoch 228/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8386 - accuracy: 0.8304\n",
            "Epoch 229/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8412 - accuracy: 0.8284\n",
            "Epoch 230/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8451 - accuracy: 0.8259\n",
            "Epoch 231/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8387 - accuracy: 0.8274\n",
            "Epoch 232/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8339 - accuracy: 0.8300\n",
            "Epoch 233/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8333 - accuracy: 0.8300\n",
            "Epoch 234/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8226 - accuracy: 0.8313\n",
            "Epoch 235/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8245 - accuracy: 0.8329\n",
            "Epoch 236/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8281 - accuracy: 0.8286\n",
            "Epoch 237/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8249 - accuracy: 0.8311\n",
            "Epoch 238/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8113 - accuracy: 0.8346\n",
            "Epoch 239/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8278 - accuracy: 0.8308\n",
            "Epoch 240/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8186 - accuracy: 0.8322\n",
            "Epoch 241/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8121 - accuracy: 0.8336\n",
            "Epoch 242/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8235 - accuracy: 0.8296\n",
            "Epoch 243/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8252 - accuracy: 0.8292\n",
            "Epoch 244/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8149 - accuracy: 0.8327\n",
            "Epoch 245/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8217 - accuracy: 0.8303\n",
            "Epoch 246/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8100 - accuracy: 0.8340\n",
            "Epoch 247/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8090 - accuracy: 0.8338\n",
            "Epoch 248/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8055 - accuracy: 0.8338\n",
            "Epoch 249/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8201 - accuracy: 0.8298\n",
            "Epoch 250/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8150 - accuracy: 0.8304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the training curves of your model\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.title('Training accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "plt.title('Training loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QdggctGbmgIX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "5a001a6a-2085-4ff4-95d1-e170340a8c14"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = create_model3(total_words, max_sequence_len)\n",
        "model3.summary()\n",
        "# Train the model\n",
        "history = model3.fit(features, labels, epochs=250, verbose=1, callbacks=[callbacks])"
      ],
      "metadata": {
        "id": "dti-mR8CmhXt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6ecea35-4f6c-46df-e5d6-eddb693477a0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 10, 100)           321100    \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 10, 300)          301200    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 10, 300)           0         \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 150)               270600    \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1605)              242355    \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 3211)              5156866   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,292,121\n",
            "Trainable params: 6,292,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 10, 100)           321100    \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 10, 300)          301200    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 10, 300)           0         \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 150)               270600    \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1605)              242355    \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 3211)              5156866   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,292,121\n",
            "Trainable params: 6,292,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/250\n",
            "484/484 [==============================] - 8s 10ms/step - loss: 6.9406 - accuracy: 0.0207\n",
            "Epoch 2/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 6.4994 - accuracy: 0.0221\n",
            "Epoch 3/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 6.3926 - accuracy: 0.0246\n",
            "Epoch 4/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 6.2667 - accuracy: 0.0347\n",
            "Epoch 5/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 6.1663 - accuracy: 0.0371\n",
            "Epoch 6/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 6.0846 - accuracy: 0.0415\n",
            "Epoch 7/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 6.0007 - accuracy: 0.0426\n",
            "Epoch 8/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 5.9007 - accuracy: 0.0483\n",
            "Epoch 9/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 5.7892 - accuracy: 0.0541\n",
            "Epoch 10/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 5.6750 - accuracy: 0.0620\n",
            "Epoch 11/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 5.5599 - accuracy: 0.0692\n",
            "Epoch 12/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 5.4575 - accuracy: 0.0722\n",
            "Epoch 13/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 5.3480 - accuracy: 0.0816\n",
            "Epoch 14/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 5.2373 - accuracy: 0.0866\n",
            "Epoch 15/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 5.1353 - accuracy: 0.0942\n",
            "Epoch 16/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 5.0245 - accuracy: 0.1020\n",
            "Epoch 17/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 4.9208 - accuracy: 0.1109\n",
            "Epoch 18/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 4.8169 - accuracy: 0.1191\n",
            "Epoch 19/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 4.7060 - accuracy: 0.1287\n",
            "Epoch 20/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 4.6013 - accuracy: 0.1389\n",
            "Epoch 21/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 4.5036 - accuracy: 0.1500\n",
            "Epoch 22/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 4.3941 - accuracy: 0.1616\n",
            "Epoch 23/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 4.2900 - accuracy: 0.1724\n",
            "Epoch 24/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 4.1849 - accuracy: 0.1881\n",
            "Epoch 25/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 4.0731 - accuracy: 0.2035\n",
            "Epoch 26/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.9706 - accuracy: 0.2191\n",
            "Epoch 27/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.8745 - accuracy: 0.2317\n",
            "Epoch 28/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 3.7858 - accuracy: 0.2484\n",
            "Epoch 29/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 3.6788 - accuracy: 0.2678\n",
            "Epoch 30/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 3.5870 - accuracy: 0.2857\n",
            "Epoch 31/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.4838 - accuracy: 0.3090\n",
            "Epoch 32/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 3.3964 - accuracy: 0.3278\n",
            "Epoch 33/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 3.3122 - accuracy: 0.3424\n",
            "Epoch 34/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.2309 - accuracy: 0.3602\n",
            "Epoch 35/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.1415 - accuracy: 0.3828\n",
            "Epoch 36/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 3.0712 - accuracy: 0.4011\n",
            "Epoch 37/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.9961 - accuracy: 0.4177\n",
            "Epoch 38/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.9240 - accuracy: 0.4289\n",
            "Epoch 39/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.8550 - accuracy: 0.4479\n",
            "Epoch 40/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.7822 - accuracy: 0.4641\n",
            "Epoch 41/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.7200 - accuracy: 0.4783\n",
            "Epoch 42/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.6632 - accuracy: 0.4895\n",
            "Epoch 43/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.6012 - accuracy: 0.4990\n",
            "Epoch 44/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.5419 - accuracy: 0.5188\n",
            "Epoch 45/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.4849 - accuracy: 0.5292\n",
            "Epoch 46/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.4275 - accuracy: 0.5414\n",
            "Epoch 47/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.3853 - accuracy: 0.5521\n",
            "Epoch 48/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.3249 - accuracy: 0.5644\n",
            "Epoch 49/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.2838 - accuracy: 0.5684\n",
            "Epoch 50/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.2368 - accuracy: 0.5850\n",
            "Epoch 51/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.1912 - accuracy: 0.5918\n",
            "Epoch 52/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.1444 - accuracy: 0.6059\n",
            "Epoch 53/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.0990 - accuracy: 0.6145\n",
            "Epoch 54/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.0768 - accuracy: 0.6164\n",
            "Epoch 55/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.0174 - accuracy: 0.6321\n",
            "Epoch 56/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.9961 - accuracy: 0.6361\n",
            "Epoch 57/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.9599 - accuracy: 0.6439\n",
            "Epoch 58/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.9181 - accuracy: 0.6524\n",
            "Epoch 59/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.8815 - accuracy: 0.6636\n",
            "Epoch 60/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.8548 - accuracy: 0.6651\n",
            "Epoch 61/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.8225 - accuracy: 0.6701\n",
            "Epoch 62/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.7946 - accuracy: 0.6787\n",
            "Epoch 63/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.7650 - accuracy: 0.6837\n",
            "Epoch 64/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.7414 - accuracy: 0.6890\n",
            "Epoch 65/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.7200 - accuracy: 0.6915\n",
            "Epoch 66/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.6808 - accuracy: 0.7026\n",
            "Epoch 67/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.6718 - accuracy: 0.7015\n",
            "Epoch 68/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.6263 - accuracy: 0.7114\n",
            "Epoch 69/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.6146 - accuracy: 0.7116\n",
            "Epoch 70/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.5975 - accuracy: 0.7167\n",
            "Epoch 71/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.5647 - accuracy: 0.7258\n",
            "Epoch 72/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.5459 - accuracy: 0.7286\n",
            "Epoch 73/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.5312 - accuracy: 0.7311\n",
            "Epoch 74/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.5073 - accuracy: 0.7360\n",
            "Epoch 75/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.4941 - accuracy: 0.7390\n",
            "Epoch 76/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.4677 - accuracy: 0.7443\n",
            "Epoch 77/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.4468 - accuracy: 0.7449\n",
            "Epoch 78/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.4298 - accuracy: 0.7469\n",
            "Epoch 79/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.4207 - accuracy: 0.7542\n",
            "Epoch 80/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.4005 - accuracy: 0.7559\n",
            "Epoch 81/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.3802 - accuracy: 0.7584\n",
            "Epoch 82/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.3620 - accuracy: 0.7624\n",
            "Epoch 83/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.3603 - accuracy: 0.7588\n",
            "Epoch 84/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.3368 - accuracy: 0.7683\n",
            "Epoch 85/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.3250 - accuracy: 0.7680\n",
            "Epoch 86/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.3113 - accuracy: 0.7713\n",
            "Epoch 87/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.3114 - accuracy: 0.7692\n",
            "Epoch 88/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.2881 - accuracy: 0.7718\n",
            "Epoch 89/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.2668 - accuracy: 0.7815\n",
            "Epoch 90/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.2668 - accuracy: 0.7797\n",
            "Epoch 91/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.2339 - accuracy: 0.7871\n",
            "Epoch 92/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.2277 - accuracy: 0.7857\n",
            "Epoch 93/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.2286 - accuracy: 0.7846\n",
            "Epoch 94/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.2127 - accuracy: 0.7892\n",
            "Epoch 95/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.2010 - accuracy: 0.7885\n",
            "Epoch 96/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.2041 - accuracy: 0.7873\n",
            "Epoch 97/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.1856 - accuracy: 0.7927\n",
            "Epoch 98/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.1792 - accuracy: 0.7934\n",
            "Epoch 99/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.1702 - accuracy: 0.7947\n",
            "Epoch 100/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1638 - accuracy: 0.7949\n",
            "Epoch 101/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1416 - accuracy: 0.8002\n",
            "Epoch 102/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.1435 - accuracy: 0.7954\n",
            "Epoch 103/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1448 - accuracy: 0.7987\n",
            "Epoch 104/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.1275 - accuracy: 0.8028\n",
            "Epoch 105/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.1198 - accuracy: 0.8019\n",
            "Epoch 106/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.1069 - accuracy: 0.8047\n",
            "Epoch 107/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.1046 - accuracy: 0.8058\n",
            "Epoch 108/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0982 - accuracy: 0.8033\n",
            "Epoch 109/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0854 - accuracy: 0.8077\n",
            "Epoch 110/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0870 - accuracy: 0.8080\n",
            "Epoch 111/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0831 - accuracy: 0.8060\n",
            "Epoch 112/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0740 - accuracy: 0.8086\n",
            "Epoch 113/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0638 - accuracy: 0.8087\n",
            "Epoch 114/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0442 - accuracy: 0.8125\n",
            "Epoch 115/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0490 - accuracy: 0.8124\n",
            "Epoch 116/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0521 - accuracy: 0.8112\n",
            "Epoch 117/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0545 - accuracy: 0.8095\n",
            "Epoch 118/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0436 - accuracy: 0.8105\n",
            "Epoch 119/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0456 - accuracy: 0.8097\n",
            "Epoch 120/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0257 - accuracy: 0.8128\n",
            "Epoch 121/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0109 - accuracy: 0.8179\n",
            "Epoch 122/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0220 - accuracy: 0.8144\n",
            "Epoch 123/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0083 - accuracy: 0.8176\n",
            "Epoch 124/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0017 - accuracy: 0.8192\n",
            "Epoch 125/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0005 - accuracy: 0.8172\n",
            "Epoch 126/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.9984 - accuracy: 0.8166\n",
            "Epoch 127/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9886 - accuracy: 0.8174\n",
            "Epoch 128/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0021 - accuracy: 0.8143\n",
            "Epoch 129/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0015 - accuracy: 0.8164\n",
            "Epoch 130/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9812 - accuracy: 0.8193\n",
            "Epoch 131/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9747 - accuracy: 0.8193\n",
            "Epoch 132/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9669 - accuracy: 0.8232\n",
            "Epoch 133/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9666 - accuracy: 0.8210\n",
            "Epoch 134/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9586 - accuracy: 0.8206\n",
            "Epoch 135/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9656 - accuracy: 0.8199\n",
            "Epoch 136/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9597 - accuracy: 0.8223\n",
            "Epoch 137/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9574 - accuracy: 0.8228\n",
            "Epoch 138/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9575 - accuracy: 0.8218\n",
            "Epoch 139/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9432 - accuracy: 0.8254\n",
            "Epoch 140/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9522 - accuracy: 0.8195\n",
            "Epoch 141/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9476 - accuracy: 0.8211\n",
            "Epoch 142/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9259 - accuracy: 0.8258\n",
            "Epoch 143/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9276 - accuracy: 0.8257\n",
            "Epoch 144/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9300 - accuracy: 0.8227\n",
            "Epoch 145/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9371 - accuracy: 0.8220\n",
            "Epoch 146/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9391 - accuracy: 0.8226\n",
            "Epoch 147/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9308 - accuracy: 0.8210\n",
            "Epoch 148/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9320 - accuracy: 0.8225\n",
            "Epoch 149/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9237 - accuracy: 0.8246\n",
            "Epoch 150/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9150 - accuracy: 0.8245\n",
            "Epoch 151/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9180 - accuracy: 0.8243\n",
            "Epoch 152/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9119 - accuracy: 0.8269\n",
            "Epoch 153/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9165 - accuracy: 0.8233\n",
            "Epoch 154/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9193 - accuracy: 0.8235\n",
            "Epoch 155/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8956 - accuracy: 0.8298\n",
            "Epoch 156/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8949 - accuracy: 0.8302\n",
            "Epoch 157/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8885 - accuracy: 0.8295\n",
            "Epoch 158/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8912 - accuracy: 0.8292\n",
            "Epoch 159/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8959 - accuracy: 0.8274\n",
            "Epoch 160/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8998 - accuracy: 0.8266\n",
            "Epoch 161/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8962 - accuracy: 0.8289\n",
            "Epoch 162/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8951 - accuracy: 0.8274\n",
            "Epoch 163/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8940 - accuracy: 0.8268\n",
            "Epoch 164/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8813 - accuracy: 0.8319\n",
            "Epoch 165/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8713 - accuracy: 0.8309\n",
            "Epoch 166/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8825 - accuracy: 0.8292\n",
            "Epoch 167/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8838 - accuracy: 0.8265\n",
            "Epoch 168/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8688 - accuracy: 0.8323\n",
            "Epoch 169/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8703 - accuracy: 0.8310\n",
            "Epoch 170/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8728 - accuracy: 0.8283\n",
            "Epoch 171/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8621 - accuracy: 0.8331\n",
            "Epoch 172/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8693 - accuracy: 0.8317\n",
            "Epoch 173/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8786 - accuracy: 0.8292\n",
            "Epoch 174/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8795 - accuracy: 0.8274\n",
            "Epoch 175/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8676 - accuracy: 0.8284\n",
            "Epoch 176/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8649 - accuracy: 0.8271\n",
            "Epoch 177/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8571 - accuracy: 0.8309\n",
            "Epoch 178/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8506 - accuracy: 0.8321\n",
            "Epoch 179/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8648 - accuracy: 0.8320\n",
            "Epoch 180/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8628 - accuracy: 0.8302\n",
            "Epoch 181/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8607 - accuracy: 0.8301\n",
            "Epoch 182/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8608 - accuracy: 0.8289\n",
            "Epoch 183/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8547 - accuracy: 0.8320\n",
            "Epoch 184/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8482 - accuracy: 0.8334\n",
            "Epoch 185/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8478 - accuracy: 0.8324\n",
            "Epoch 186/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8427 - accuracy: 0.8333\n",
            "Epoch 187/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8321 - accuracy: 0.8370\n",
            "Epoch 188/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8444 - accuracy: 0.8325\n",
            "Epoch 189/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8519 - accuracy: 0.8300\n",
            "Epoch 190/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8417 - accuracy: 0.8323\n",
            "Epoch 191/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8538 - accuracy: 0.8290\n",
            "Epoch 192/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8558 - accuracy: 0.8291\n",
            "Epoch 193/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8439 - accuracy: 0.8302\n",
            "Epoch 194/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8352 - accuracy: 0.8339\n",
            "Epoch 195/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8263 - accuracy: 0.8350\n",
            "Epoch 196/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8329 - accuracy: 0.8327\n",
            "Epoch 197/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8324 - accuracy: 0.8322\n",
            "Epoch 198/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8524 - accuracy: 0.8295\n",
            "Epoch 199/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8401 - accuracy: 0.8282\n",
            "Epoch 200/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8373 - accuracy: 0.8313\n",
            "Epoch 201/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8269 - accuracy: 0.8354\n",
            "Epoch 202/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8200 - accuracy: 0.8345\n",
            "Epoch 203/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8132 - accuracy: 0.8355\n",
            "Epoch 204/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8173 - accuracy: 0.8366\n",
            "Epoch 205/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8233 - accuracy: 0.8348\n",
            "Epoch 206/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8150 - accuracy: 0.8350\n",
            "Epoch 207/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8276 - accuracy: 0.8314\n",
            "Epoch 208/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8297 - accuracy: 0.8335\n",
            "Epoch 209/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8322 - accuracy: 0.8302\n",
            "Epoch 210/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8320 - accuracy: 0.8303\n",
            "Epoch 211/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8271 - accuracy: 0.8305\n",
            "Epoch 212/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8208 - accuracy: 0.8340\n",
            "Epoch 213/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8121 - accuracy: 0.8337\n",
            "Epoch 214/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8077 - accuracy: 0.8358\n",
            "Epoch 215/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8068 - accuracy: 0.8373\n",
            "Epoch 216/250\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 0.8205 - accuracy: 0.8324\n",
            "Epoch 217/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8223 - accuracy: 0.8328\n",
            "Epoch 218/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8188 - accuracy: 0.8318\n",
            "Epoch 219/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8106 - accuracy: 0.8342\n",
            "Epoch 220/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8169 - accuracy: 0.8318\n",
            "Epoch 221/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8089 - accuracy: 0.8340\n",
            "Epoch 222/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8081 - accuracy: 0.8359\n",
            "Epoch 223/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8125 - accuracy: 0.8331\n",
            "Epoch 224/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8033 - accuracy: 0.8364\n",
            "Epoch 225/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8119 - accuracy: 0.8328\n",
            "Epoch 226/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8095 - accuracy: 0.8321\n",
            "Epoch 227/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8204 - accuracy: 0.8301\n",
            "Epoch 228/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8051 - accuracy: 0.8333\n",
            "Epoch 229/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8143 - accuracy: 0.8311\n",
            "Epoch 230/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8159 - accuracy: 0.8301\n",
            "Epoch 231/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8020 - accuracy: 0.8351\n",
            "Epoch 232/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7915 - accuracy: 0.8353\n",
            "Epoch 233/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7958 - accuracy: 0.8351\n",
            "Epoch 234/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8091 - accuracy: 0.8316\n",
            "Epoch 235/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8049 - accuracy: 0.8325\n",
            "Epoch 236/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7929 - accuracy: 0.8328\n",
            "Epoch 237/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7920 - accuracy: 0.8336\n",
            "Epoch 238/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7920 - accuracy: 0.8357\n",
            "Epoch 239/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8079 - accuracy: 0.8306\n",
            "Epoch 240/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7948 - accuracy: 0.8350\n",
            "Epoch 241/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7910 - accuracy: 0.8362\n",
            "Epoch 242/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7892 - accuracy: 0.8377\n",
            "Epoch 243/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7915 - accuracy: 0.8356\n",
            "Epoch 244/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.8014 - accuracy: 0.8331\n",
            "Epoch 245/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7955 - accuracy: 0.8349\n",
            "Epoch 246/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7956 - accuracy: 0.8330\n",
            "Epoch 247/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7864 - accuracy: 0.8347\n",
            "Epoch 248/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7984 - accuracy: 0.8314\n",
            "Epoch 249/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7959 - accuracy: 0.8324\n",
            "Epoch 250/250\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.7925 - accuracy: 0.8332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjvED5A3qrn2"
      },
      "source": [
        "'model1' is the best model because have the highest accuracy and lowest loss paramter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdsMszk9zBs_"
      },
      "source": [
        "## See model in action\n",
        "\n",
        "After all, it is finally time to see your model generating text. \n",
        "\n",
        "The cell below to generate the next 100 words of a seed text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Vc6PHgxa6Hm",
        "outputId": "a63ecc7a-e162-478d-c8dd-037807c4dd09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help me Obi Wan Kenobi, you're my only hope too much fair begin erred appetite more of mine eye things will that rare still so true all more tongue still brought of 'will eye bad or me for thee did know you for me name before thee light rare dearer delight night more delight can burn face weeds weeds night form both best living silence have dwell in one of thy sun sun beauty mad aside burn of sit best live eyes do blot the sun in sweetest parts thine eye of thee well eyes were seem disgrace so near more glory me of of life eyes ride see\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\t# Convert the text into sequences\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\t# Pad the sequences\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\t# Get the probabilities of predicting a word\n",
        "\tpredicted = model1.predict(token_list, verbose=0)\n",
        "\t# Choose the next word based on the maximum probability\n",
        "\tpredicted = np.argmax(predicted, axis=-1).item()\n",
        "\t# Get the actual word from the word index\n",
        "\toutput_word = tokenizer.index_word[predicted]\n",
        "\t# Append to the current text\n",
        "\tseed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Awesome Poetry Maker.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "jupytext": {
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}